{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vCdgEhHp-DMY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reproducibility\n",
    "def set_random_seeds(seed: int = 42) -> None:\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    return device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0TNm1VB_-dIO"
   },
   "outputs": [],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    \"\"\"Custom dataset for sentiment analysis.\"\"\"\n",
    "\n",
    "    def __init__(self, texts: List[str], labels: List[int], tokenizer, max_length: int = 128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TnSMqmED-fba"
   },
   "outputs": [],
   "source": [
    "class SentimentAnalyzer(nn.Module):\n",
    "    \"\"\"Transformer-based sentiment analysis model.\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = 'distilbert-base-uncased', num_classes: int = 3, dropout_rate: float = 0.3):\n",
    "        super(SentimentAnalyzer, self).__init__()\n",
    "        self.transformer = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.transformer.config.hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state.mean(dim=1)  # CLS token\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ycQekne-hbB"
   },
   "outputs": [],
   "source": [
    "def load_and_prepare_data():\n",
    "    df_main = pd.read_csv(\"cleaned_dataset.csv\")\n",
    "    df_ref = pd.read_csv('labels_reference_cleaned.csv')\n",
    "\n",
    "    sentiment_map = {'Negative': 0, 'Neutral': 1, 'Positive': 2}\n",
    "\n",
    "    # ---- MAIN DATASET ----\n",
    "    df_main = df_main[['Text_Cleaned', 'Sentiment']].dropna()\n",
    "    df_main['sentiment_label'] = df_main['Sentiment'].map(sentiment_map)\n",
    "    df_main = df_main[['Text_Cleaned', 'sentiment_label']]\n",
    "    df_main = df_main.rename(columns={'Text_Cleaned': 'text'})\n",
    "\n",
    "    # ---- REF DATASET ----\n",
    "    df_ref = df_ref[['clean_text', 'sentiment_label']].dropna()\n",
    "    df_ref = df_ref.rename(columns={'clean_text': 'text'})\n",
    "\n",
    "    # ---- COMBINE ----\n",
    "    df = pd.concat([df_main, df_ref], ignore_index=True)\n",
    "\n",
    "    # ---- CLEANING PIPELINE ----\n",
    "\n",
    "    # ensure string\n",
    "    df['text'] = df['text'].astype(str)\n",
    "\n",
    "    # remove duplicates\n",
    "    df = df.drop_duplicates(subset='text')\n",
    "\n",
    "    # remove very short sentences\n",
    "    df = df[df['text'].str.split().str.len() > 5]\n",
    "\n",
    "    # remove junk twitter words\n",
    "    df = df[~df['text'].str.contains(\"haha|hahah|lol|powerblog|omg\", case=False)]\n",
    "\n",
    "    # remove extremely short characters\n",
    "    df = df[df['text'].str.len() > 25]\n",
    "\n",
    "    # remove invalid labels\n",
    "    df = df[df['sentiment_label'].isin([0,1,2])]\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    print(\"Final cleaned dataset:\", df.shape)\n",
    "    print(df['sentiment_label'].value_counts())\n",
    "\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XHRU54eh-jmM"
   },
   "outputs": [],
   "source": [
    "def create_data_loaders(df, tokenizer, test_size=0.2, val_size=0.1, batch_size=16, max_length=192):\n",
    "\n",
    "    X = df['text'].tolist()\n",
    "    y = df['sentiment_label'].tolist()\n",
    "\n",
    "    # Train + Test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=42\n",
    "    )\n",
    "\n",
    "    # Train + Validation split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=val_size, stratify=y_train, random_state=42\n",
    "    )\n",
    "\n",
    "    train_dataset = SentimentDataset(X_train, y_train, tokenizer, max_length)\n",
    "    val_dataset = SentimentDataset(X_val, y_val, tokenizer, max_length)\n",
    "    test_dataset = SentimentDataset(X_test, y_test, tokenizer, max_length)\n",
    "\n",
    "    return (\n",
    "        DataLoader(train_dataset, batch_size=batch_size, shuffle=True),\n",
    "        DataLoader(val_dataset, batch_size=batch_size, shuffle=False),\n",
    "        DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xeoC1qEe-liF"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=20, lr=5e-6):\n",
    "\n",
    "    device = get_device()\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Freeze transformer for warmup\n",
    "    for param in model.transformer.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    class_counts = torch.tensor(df['sentiment_label'].value_counts().sort_index().values)\n",
    "    weights = 1.0 / class_counts\n",
    "    weights = weights / weights.sum()\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights.to(device))\n",
    "\n",
    "\n",
    "    losses = []\n",
    "    best_val = 0\n",
    "    patience = 3\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Unfreeze after 2 epochs\n",
    "        if epoch == 2:\n",
    "            for param in model.transformer.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        # ---- TRAIN ----\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_acc = correct / total\n",
    "        losses.append(total_loss / len(train_loader))\n",
    "\n",
    "        # ---- VALIDATION ----\n",
    "        model.eval()\n",
    "        val_correct, val_total = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                ids = batch['input_ids'].to(device)\n",
    "                mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "\n",
    "                outputs = model(ids, mask)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        print(f\"Epoch {epoch+1} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # ---- EARLY STOPPING ----\n",
    "        if val_acc > best_val:\n",
    "            best_val = val_acc\n",
    "            counter = 0\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IAjTi-fT-nhf"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(true_labels, preds):\n",
    "    cm = confusion_matrix(true_labels, preds)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Negative','Neutral','Positive'],\n",
    "                yticklabels=['Negative','Neutral','Positive'])\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_training_loss(losses):\n",
    "    plt.plot(losses, 'b-')\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ymoQhy4X-pa4"
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(model, tokenizer, text, device):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=192,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        pred = torch.argmax(probs, dim=1).item()\n",
    "\n",
    "    sentiment_map = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n",
    "    return sentiment_map[pred], probs[0].cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "1d2d16f9577b48b0a038b53baa16e9c6",
      "92d998fd215f4fb1b57d3777b24f5daa",
      "ed1936c0c90a4b9aa8c7912c30b5cca8",
      "59d78058b26746559d85753a8d14ceb8",
      "818ba800b7254979942f79b5747ac530",
      "bf9fd910b2e343828bbc1048aaf43c0d",
      "c2b265ba967c4b41adb5b49b7a997c79",
      "65c0906c400e43b2914d80d5447a6991",
      "4f60770c864a4b868676f9e65851eda4",
      "ad3bc54cfe93418ca0ff4fb2cd2a3c0c",
      "624d685aa4014da085635277879066a2",
      "d61e029618944cbdb7ad73acf79b4b9b",
      "2625bda662894b08b5f0ece262f28b84",
      "909a87d3166a404aafc8fc98bd75ec54",
      "9b9c05e5254d41c1a5e55ad5a4a5fb94",
      "978142c528eb4d8cbc9f65d5acab2f70",
      "59fdadfeedc84d8eb68beb016f0aeea3",
      "a13ba814ef314fe1a6d22863534ec394",
      "d5a8799745ed44b3aa96eade5fe24bd1",
      "597bd2b35feb4fa38289910d388ca4c0",
      "378ab028320040548d196b957b3b7bfa",
      "e07ca22ef0ee456e93b0eca82d0de590",
      "85a23446591946ec8787fa12455cdc4b",
      "43878dd875e84aefa951558baec1ef82",
      "c415ea8ec7cc43dcbd05d1ba55d05a70",
      "8d6a0068916f46f49868ef2356195d1a",
      "897c459366d740979e6ed7037e88f61d",
      "1c7a2c060c074d78bf8328dd2e445ee3",
      "d0d8656fcd3c48aead5ac8da108987af",
      "f8ead99732cc46beaa73f787b77f4655",
      "93b1c20d505243bf82f7514b707694c3",
      "364589bcdffa4061b4d90f4a723dcf5b",
      "2210f2e23a81497c912162064dcbe334",
      "a6f52ba7ec2e468da3c21b8b6b5984dc",
      "50cb0bee73174a3d88ea3578fdac4e90",
      "8f8ed53a40c247069f2514c47a86f3a0",
      "d6613f21a9e344e8806dc397c58e033b",
      "0accbf6ed6fe4e6d888c16aa85e69228",
      "9f456dc88cac4dfd82a474e8071788d4",
      "0ed867461a5845bda385d03a706a8cbc",
      "82fa0a8b58494be18efa241e534f5ec2",
      "64f851af011e4f4f91cb0c9523b780ec",
      "7913a38f7043482fab0674edaa5742b4",
      "631285b96fe54948b2e390cb70e30983",
      "bd540d64a673451aa8b3019b4bb0687e",
      "d268ae579e6c43348c5ed2952e88d627",
      "e33a0a2a8995419384b46186bbf93f8a",
      "99dd66f3045141d68275f5617c9b6c67",
      "663de02bafee40a5b05467575530ffd3",
      "8ef952767d4640608df318d4666e108a",
      "3d563057569a491282f3594b900773ce",
      "4d9e13152b1e4788a2b5fcb36dd9bb0a",
      "a3ea2f0f12a64e0f824744f56bd75569",
      "bb56c18b7db44899bc82e80b097e72d4",
      "555dc9be473e4463af3dfaadb1e43237",
      "22b7d5a791944cc2b784f2715ee125a8",
      "160d501320a54a04bc4c9cdd32e504c4",
      "4006e9bc38384c959e8bed7c89d8277a",
      "0fb420c547274c3ca2fb2ca78b1b4af9",
      "09141839563d4b1c8ffaef50d1f6438b",
      "3fbdc8afec3c468989304afa42f1a39b",
      "90b07aa24b634a468ad54c8a632c8617",
      "59fcbcd34b18444f94a5627e9374dbec",
      "065d455cfc254ff0b1651da72a89e9eb",
      "21bc9d1213874730b4590209cb703e89",
      "7a70b4300f1e47abb43a4f8d62000e95"
     ]
    },
    "id": "QGdBYb0V-rFp",
    "outputId": "e242f651-3249-40c8-fc50-7ee0a6606113"
   },
   "outputs": [],
   "source": [
    "set_random_seeds(42)\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    device = get_device()\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            ids = batch['input_ids'].to(device)\n",
    "            mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            outputs = model(ids, mask)\n",
    "            batch_preds = torch.argmax(outputs, dim=1)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            preds.extend(batch_preds.cpu().numpy())\n",
    "    return true_labels, preds\n",
    "\n",
    "df = load_and_prepare_data()\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = SentimentAnalyzer(model_name=model_name, num_classes=3)\n",
    "\n",
    "train_loader, val_loader, test_loader = create_data_loaders(df, tokenizer)\n",
    "\n",
    "losses = train_model(model, train_loader, val_loader, num_epochs=20)\n",
    "\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "print(\"Loaded best model\")\n",
    "\n",
    "true_labels, preds = evaluate_model(model, test_loader)\n",
    "\n",
    "plot_confusion_matrix(true_labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TCI2nSIa-ssA",
    "outputId": "c4c115cd-535f-4457-a8e9-f94a8216033d"
   },
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "samples = [\n",
    "    \"I love this amazing product! It's fantastic!\",\n",
    "    \"This is terrible. I hate it so much.\",\n",
    "    \"It's okay, nothing special but not bad either.\"\n",
    "]\n",
    "\n",
    "for s in samples:\n",
    "    sentiment, probs = predict_sentiment(model, tokenizer, s, device)\n",
    "    print(f\"Text: {s}\")\n",
    "    print(f\"Predicted: {sentiment} | Probabilities: {probs}\")\n",
    "    print(\"-\"*40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B1Kr3-yG-vDG",
    "outputId": "f186f2cb-23f0-4365-bab4-4f6f64569617"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "true_labels, preds = evaluate_model(model, test_loader)\n",
    "\n",
    "print(\"\\nFINAL TEST ACCURACY:\", accuracy_score(true_labels, preds))\n",
    "print(\"\\nCLASSIFICATION REPORT:\\n\")\n",
    "print(classification_report(true_labels, preds, target_names=['Negative','Neutral','Positive']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HUMXTpn4-yje",
    "outputId": "a377400a-3cf9-418d-874d-ee97633abfee"
   },
   "outputs": [],
   "source": [
    "text = \"I absolutely hate this product\"\n",
    "sentiment, probs = predict_sentiment(model, tokenizer, text, device)\n",
    "print(sentiment, probs)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
